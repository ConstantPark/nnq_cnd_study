# Study-Neural Network Quantization & Compact Networks Design

This is a repository of Facebook Group *AI Robotics KR*.<br>

*nnq_mc_study* stands for Neural Network Quantization &amp; Compact Networks Design Study.

It will be focusing on paper reviews for deep neural networks, model compression, compact network design, and Quantization.

Online Study supported by AI Robotics KR group have been ongoing since September 1st.

## Prerequisite

- Basic Understanding for deep learning algorithms like DNN, RNN, CNN is preferred
- Passion for learning
- Persistence
- Motivation


## Learning Objectives

- Deep understanding for Deep Learning Quantization & Compact Networks Design Algorithms


## How to Study

- Online Presentation
- Q & A

------------------------------
## Participants:

**Slack** : @Hwigeon Oh, @Seojin Kim, @DongJunMin, @이경준, @Hyunwoo Kim, @Constant, @임병학, @KimYoungBin, @Sanggun Kim, @martin, @Joh, @김석중, @Yongwoo Kim, @MinSeop Lee, @Woz.D, @inwoong.lee (이인웅), @Hoyeolchoi, @Bochan Kim, @Young Seok Kim, @taehkim, @Seongmock Yoo, @Mike.Oh, @최승호, @Davidlee, @Stella Yang, @sejungkwon, @Jaeyoung Lee, @Hyungjun Kim, @tae-ha, @Jeonghoon.

------------------------------------

**GitHub** : 

## Contributors:

**Main Study Learder**: Jeonghoon Kim(GitHub:IntelligenceDatum).

**Compact Networks Design Leader**: Seo Yeon Stella Yang(GitHub:howtowhy).

**Presenter**:Jeonghoon Kim, Stella Yang, Sanggun Kim, Hyunwoo Kim, Seojin Kim, Hwigeon Oh, Seojin Kim, Seokjoong Kim, Martin Hwang, Youngbin Kim, Sang-soo Park, Jaeyoung Lee, Yongwoo Kim, Hyungjun Kim, Sejung Kwon, 이경준, Bochan Kim, 이인웅. 

------------------------------------

## Presentation with Video :

Neural Network Quantization & Compact Network Development Study

### Week1: Introduction of NNQ&CND
**Title: A Piece of Weight**  
Presentor: 김정훈 (Jeonghoon Kim)  
PPT: https://drive.google.com/open?id=1RQAiIFX7wOUMiZXPCIZbXb_6DtLlV38e  
Video: https://youtu.be/pohMFz-uQJ0  


### Week2: BNN & MobileNet + CND Overview (Makeup class)  
**Title: Compact Network Development Overview**  
Presentor: Stella Yang  
Video: https://youtu.be/R3pE-pGBbBg  
PPT: https://drive.google.com/open?id=1bTy68uO1Ta4tJLYDLA7d6GJfRx1YbcM4  

**Paper: Binarized Neural Networks:Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1**  
Presentor: 김정훈 (Jeonghoon Kim)  
Video:  https://youtu.be/n89CsZpZcNk  
PPT:  https://drive.google.com/open?id=1DoeGj-goeI5WMIu5LPTQ6aFZ2czl7eNP  

**Paper: MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications**  
Presentor:  김상근 (Sanggun Kim)  
Video: https://youtu.be/GyQUBLDQEJI  
PPT: https://drive.google.com/open?id=1oQI8Pv7N66pZHflx0CyMIyahhbA-Dce7  
  

------------------------------------
## Schedule (Presentation List):

| Week         | Subject                                                                                            | Presenter       |
|--------------|----------------------------------------------------------------------------------------------------|-----------------|
| Week 1  |1. Introduction <br /> 2. Introduction. |  1.Jeonghoon Kim<br />2.Stella Yang|
| Week 2  |1. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1.<br /> 2. Mobilenets: Efficient convolutional neural networks for mobile vision applications. |1.Jeonghoon Kim<br />2.Sanggun Kim|
| Week 3  |1.Finn: A framework for fast, scalable binarized neural network inference.<br />2. MobileNetV2: Inverted Residuals and Linear Bottlenecks| 1.Hyunwoo Kim<br />2.Seojin Kim  |
| Week 4  |1.XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks.<br />2. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size. |1.Hwigeon Oh<br />2.Martin Hwang |
| Week 5  |1.BNN+: Improved binary network training.<br />2. Squeezenext: Hardware-aware neural network design. |1.Youngbin Kim<br /> 2.Sang-soo Park|
| Week 6  |1.Loss-aware binarization of deep networks.<br />	2. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.|1.Sanggun Kim<br />2.	Sang-soo Park|
| Week 7  |1. Loss-aware binarization of deep networks.<br />2. Scalpel: Customizing dnn pruning to the underlying hardware parallelism.|1.Youngbin Kim<br />2.Sang-soo Park|
| Week 8  |1.Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.<br />2.ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices.|1.Yongwoo Kim<br />2.Jaeyoung Lee|
| Week 9  |1.Lq-nets: Learned quantization for highly accurate and compact deep neural networks.<br />2. Model compression via distillation and quantization.|1.Hyungjun Kim<br />2. Seokjoong Kim|
| Week 10  |1. Alternating Multi-bit Quantization for Recurrent Neural Networks.<br />	2. Densely Connected Convolutional Networks.|1.Eunhui Kim<br />2.이경준|
| Week 11  |1.TBD<br />2. All You Need is a Few Shifts: Designing Efficient Convolutional Neural Networks for Image Classification.|1.Sejung Kwon<br />2.Stella Yang|
| Week 12  |1.Analysis of Quantized Models.<br />2.EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.	|1.Bochan Kim<br />2.	Martin Hwang|    
| Week 13  |1.Learning to quantize deep networks by optimizing quantization intervals with task loss.<br />2. Amc: Automl for model compression and acceleration on mobile devices.|1.이인웅<br />2.Seokjoong Kim|
