# Study-Neural Network Quantization & Model Compression

This is a repository of Facebook Group *AI Robotics KR*.<br>

*nnq_mc_study* stands for Neural Network Quantization &amp; Model Compression Study.

It will be focusing on paper reviews for deep neural networks, model compression, and Quantization.

Online Study supported by AI Robotics KR group will be held soon.

## Prerequisite

- Basic Understanding for deep learning algorithms like DNN, RNN, CNN is preferred
- Passion for learning
- Persistence


## Learning Objectives

- Deep understanding for Deep Learning Quantization & Model Compression Algorithms


## How to Study

- Online Presentation
- Q & A

---
## Participants:

**Slack** : @Hwigeon Oh, @Seojin Kim, @DongJunMin, @이경준, @Hyunwoo Kim, @Constant, @임병학, @KimYoungBin, @Sanggun Kim, @martin, @Joh, @김석중, @Yongwoo Kim, @MinSeop Lee, @Woz.D, @inwoong.lee (이인웅), @Hoyeolchoi, @Bochan Kim, @Young Seok Kim, @taehkim, @Seongmock Yoo, @Mike.Oh, @최승호, @Davidlee, @Stella Yang, @sejungkwon, @Jaeyoung Lee, @Hyungjun Kim, @Jeonghoon.

**GitHub** : 

## Contributors:

**Main Study Learder**: Jeonghoon Kim(GitHub:IntelligenceDatum).

**Model Compression Leader**: Seo Yeon Stella Yang(GitHub:howtowhy).

**Presentation List**:
 - **Jeonghoon Kim(GitHub:IntelligenceDatum)**: Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. "Binaryconnect: Training deep neural networks with binary weights during propagations." Advances in neural information processing systems. 2015.
 - **Youngbin Kim(GitHub:dudqls1994)**: Darabi, Sajad, et al. “BNN+: Improved binary network training.” arXiv preprint arXiv:1812.11800 (2018). 
 - **Yongwoo Kim**: Zhou, Shuchang, et al. “Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.” arXiv preprint arXiv:1606.06160 (2016).
 - **Hyunwoo Kim**: Yonekawa, Haruyoshi, and Hiroki Nakahara. “On-chip memory based binarized convolutional deep neural network applying batch normalization free technique on an fpga.” 2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW). IEEE, 2017. 
 - **Hyunwoo Kim**: Umuroglu, Yaman, et al. “Finn: A framework for fast, scalable binarized neural network inference.” Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 2017.
 - **Seokjoong Kim**: He, Yihui, et al. “Amc: Automl for model compression and acceleration on mobile devices.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.
 - **Seokjoong Kim**: Polino, Antonio, Razvan Pascanu, and Dan Alistarh. "Model compression via distillation and quantization." ICLR2018 Conference Paper arXiv:1802.05668 (2018).
 - **Sanggun Kim(GitHub:dldldlfma)**: Han, Song, Huizi Mao, and William J. Dally. "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding." arXiv preprint arXiv:1510.00149 (2015).
 - **Sang-soo Park(GitHub:constant)**: Yu, Jiecao, et al. "Scalpel: Customizing dnn pruning to the underlying hardware parallelism." 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA): 548-560.
 - **Hyungjun Kim**: Zhang, Dongqing, et al. “Lq-nets: Learned quantization for highly accurate and compact deep neural networks.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.

## Schedule


| Week      | Subject  | Presenter |
| ------------ | --------- | :---: |
| Week 1,  | |  |

---

