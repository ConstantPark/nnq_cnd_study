# Study-Neural Network Quantization & Model Compression

This is a repository of Facebook Group *AI Robotics KR*.<br>

*nnq_mc_study* stands for Neural Network Quantization &amp; Model Compression Study.

It will be focusing on paper reviews for deep neural networks, model compression, and Quantization.

Online Study supported by AI Robotics KR group will be held on **September 1st**.

## Prerequisite

- Basic Understanding for deep learning algorithms like DNN, RNN, CNN is preferred
- Passion for learning
- Persistence


## Learning Objectives

- Deep understanding for Deep Learning Quantization & Model Compression Algorithms


## How to Study

- Online Presentation
- Q & A

------------------------------
## Participants:

**Slack** : @Hwigeon Oh, @Seojin Kim, @DongJunMin, @이경준, @Hyunwoo Kim, @Constant, @임병학, @KimYoungBin, @Sanggun Kim, @martin, @Joh, @김석중, @Yongwoo Kim, @MinSeop Lee, @Woz.D, @inwoong.lee (이인웅), @Hoyeolchoi, @Bochan Kim, @Young Seok Kim, @taehkim, @Seongmock Yoo, @Mike.Oh, @최승호, @Davidlee, @Stella Yang, @sejungkwon, @Jaeyoung Lee, @Hyungjun Kim, @tae-ha, @Jeonghoon.

------------------------------------

**GitHub** : 

## Contributors:

**Main Study Learder**: Jeonghoon Kim(GitHub:IntelligenceDatum).

**Model Compression Leader**: Seo Yeon Stella Yang(GitHub:howtowhy).

**Presenter**:Jeonghoon Kim, Stella Yang, Sanggun Kim, Hyunwoo Kim, Seojin Kim, Hwigeon Oh, Seojin Kim, Seokjoong Kim, Martin Hwang, Youngbin Kim, Sang-soo Park, Jaeyoung Lee, Yongwoo Kim, Hyungjun Kim, Sejung Kwon, 이경준, Bochan Kim, 이인웅. 

## Schedule (Presentation List):

| Week         | Subject                                                                                            | Presenter       |
|--------------|----------------------------------------------------------------------------------------------------|-----------------|
| Week 1  |1. Introduction <br /> 2. Introduction. |  1.Jeonghoon Kim<br />2.Stella Yang|
| Week 2  |1. Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1.<br /> 2. Mobilenets: Efficient convolutional neural networks for mobile vision applications. |1.Jeonghoon Kim<br />2.Sanggun Kim|
| Week 3  |1.Finn: A framework for fast, scalable binarized neural network inference.<br />2. MobileNetV2: Inverted Residuals and Linear Bottlenecks| 1.Hyunwoo Kim<br />2.Seojin Kim  |
| Week 4  |1.XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks.<br />2. Model compression via distillation and quantization. |1.Hwigeon Oh<br />2.Seokjoong Kim |
| Week 5  |1.Amc: Automl for model compression and acceleration on mobile devices.<br />2. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size. |1.Seokjoong Kim<br /> 2.Martin Hwang|
| Week 6  |1. BNN+: Improved binary network training.<br />	2. Squeezenext: Hardware-aware neural network design.|1.Youngbin Kim<br />2.	Sang-soo Park|
| Week 7  |1. Loss-aware binarization of deep networks.<br />2. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.|1.Jaeyoung Lee<br />2.Sanggun Kim|
| Week 8  |1.Loss-aware weight quantization of deep networks.<br />	2. Scalpel: Customizing dnn pruning to the underlying hardware parallelism.|1.Youngbin Kim<br />2. Sang-soo Park|
| Week 9  |1.Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.<br />2.ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices.|1.Yongwoo Kim<br />2.Jaeyoung Lee|
| Week 10  |1.Lq-nets: Learned quantization for highly accurate and compact deep neural networks.<br />2. Model compression via distillation and quantization.|1.Hyungjun Kim<br />2. Seokjoong Kim|
| Week 11  |1. Alternating Multi-bit Quantization for Recurrent Neural Networks.<br />	2. Densely Connected Convolutional Networks.|1.Sejung Kwon. (일정 및 내용 협의 필요)<br />2.이경준|
| Week 12  |1.Deeptwist: Learning model compression via occasional weight distortion.<br />2. All You Need is a Few Shifts: Designing Efficient Convolutional Neural Networks for Image Classification.|1.Sejung Kwon (일정 및 내용 협의 필요)<br />2.Stella Yang|
| Week 13  |1.Analysis of Quantized Models.<br />2.EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.	|1.Bochan Kim<br />2.	Martin Hwang|    
| Week 14  |1.Learning to quantize deep networks by optimizing quantization intervals with task loss.<br />2. Amc: Automl for model compression and acceleration on mobile devices.|1.이인웅<br />2.Seokjoong Kim|
