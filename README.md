# Study-Neural Network Quantization & Model Compression

This is a repository of Facebook Group *AI Robotics KR*.<br>

*nnq_mc_study* stands for Neural Network Quantization &amp; Model Compression Study.

It will be focusing on paper reviews for deep neural networks, model compression, and Quantization.

Online Study supported by AI Robotics KR group will be held soon.

## Prerequisite

- Basic Understanding for deep learning algorithms like DNN, RNN, CNN is preferred
- Passion for learning
- Persistence


## Learning Objectives

- Deep understanding for Deep Learning Quantization & Model Compression Algorithms


## How to Study

- Online Presentation
- Q & A

---
## Participants:

**Slack** : @Hwigeon Oh, @Seojin Kim, @DongJunMin, @이경준, @Hyunwoo Kim, @Constant, @임병학, @KimYoungBin, @Sanggun Kim, @martin, @Joh, @김석중, @Yongwoo Kim, @MinSeop Lee, @Woz.D, @inwoong.lee (이인웅), @Hoyeolchoi, @Bochan Kim, @Young Seok Kim, @taehkim, @Seongmock Yoo, @Mike.Oh, @최승호, @Davidlee, @Stella Yang, @sejungkwon, @Jaeyoung Lee, @Hyungjun Kim, @Jeonghoon.

**GitHub** : 

## Contributors:

**Main Study Learder**: Jeonghoon Kim(GitHub:IntelligenceDatum).

**Model Compression Leader**: Seo Yeon Stella Yang(GitHub:howtowhy).

**Presentation List**:
 - **Jeonghoon Kim(GitHub:IntelligenceDatum)**: Courbariaux, Matthieu, Yoshua Bengio, and Jean-Pierre David. "Binaryconnect: Training deep neural networks with binary weights during propagations." Advances in neural information processing systems. 2015.
 - **Sanggun Kim(GitHub:dldldlfma)**: Han, Song, Huizi Mao, and William J. Dally. "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding." arXiv preprint arXiv:1510.00149 (2015).
 - **Yongwoo Kim**: Zhou, Shuchang, et al. “Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.” arXiv preprint arXiv:1606.06160 (2016).
  - **Hyunwoo Kim**: Umuroglu, Yaman, et al. “Finn: A framework for fast, scalable binarized neural network inference.” Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 2017.
 - **Sang-soo Park(GitHub:constant)**: Yu, Jiecao, et al. "Scalpel: Customizing dnn pruning to the underlying hardware parallelism." 2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA): 548-560.
 - **Youngbin Kim(GitHub:dudqls1994)**: Darabi, Sajad, et al. “BNN+: Improved binary network training.” arXiv preprint arXiv:1812.11800 (2018). 
 - **Hyungjun Kim**: Zhang, Dongqing, et al. “Lq-nets: Learned quantization for highly accurate and compact deep neural networks.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.
 - **Seokjoong Kim**: Polino, Antonio, Razvan Pascanu, and Dan Alistarh. "Model compression via distillation and quantization." ICLR2018 Conference Paper arXiv:1802.05668 (2018).
 - **Seokjoong Kim**: He, Yihui, et al. “Amc: Automl for model compression and acceleration on mobile devices.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.


## Schedule


| Week    | Subject  | Presenter |
|---------|:---------------------------------------------------------------------------------------------------------:|-------------:|
| Week 1  |1. Binaryconnect: Training deep neural networks with binary weights during propagations. | Jeonghoon Kim|
|         |2. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. |Sanggun Kim|
| Week 2  |1. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients.|Yongwoo Kim |
|         |2. Finn: A framework for fast, scalable binarized neural network inference. |Hyunwoo Kim|
| Week 3  |1.Scalpel: Customizing dnn pruning to the underlying hardware parallelism. |Sang-soo Park |
|         |2. BNN+: Improved binary network training. |Youngbin Kim|
| Week 4  |1.Lq-nets: Learned quantization for highly accurate and compact deep neural networks. |Hyungjun Kim |
|         |2. Model compression via distillation and quantization. |Seokjoong Kim|
| Week 5  |1.Amc: Automl for model compression and acceleration on mobile devices. |Seokjoong Kim |
|         |2. TBD|TBD|

